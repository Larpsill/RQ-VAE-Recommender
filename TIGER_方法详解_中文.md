# TIGERæ–¹æ³•è¯¦è§£ (RQ-VAE Recommender)

## ğŸ“š æ¦‚è¿°

è¿™ä¸ªé¡¹ç›®æ˜¯è®ºæ–‡ **"Recommender Systems with Generative Retrieval"** (TIGER) çš„PyTorchå®ç°ã€‚è¯¥æ–¹æ³•ä½¿ç”¨åŸºäºRQ-VAEçš„è¯­ä¹‰IDæ¥æ„å»ºç”Ÿæˆå¼æ£€ç´¢æ¨èç³»ç»Ÿã€‚

## ğŸ—ï¸ æ ¸å¿ƒæ€æƒ³

TIGERçš„æ ¸å¿ƒæ€æƒ³æ˜¯å°†æ¨èé—®é¢˜è½¬åŒ–ä¸ºä¸€ä¸ª**åºåˆ—ç”Ÿæˆé—®é¢˜**ï¼š

1. **è¯­ä¹‰IDç”Ÿæˆ**ï¼šä½¿ç”¨RQ-VAEå°†æ¯ä¸ªç‰©å“ç¼–ç æˆä¸€ä¸ªè¯­ä¹‰IDå…ƒç»„ï¼ˆä¾‹å¦‚ [c1, c2, c3]ï¼‰
2. **åºåˆ—é¢„æµ‹**ï¼šè®­ç»ƒä¸€ä¸ªTransformeræ¨¡å‹ï¼ŒåŸºäºç”¨æˆ·çš„å†å²äº¤äº’åºåˆ—ï¼Œè‡ªå›å½’åœ°ç”Ÿæˆä¸‹ä¸€ä¸ªç‰©å“çš„è¯­ä¹‰ID

## ğŸ“ é¡¹ç›®ç»“æ„ä¸é‡ç‚¹æ–‡ä»¶

```
RQ-VAE-Recommender/
â”œâ”€â”€ train_rqvae.py          â­ ç¬¬ä¸€é˜¶æ®µï¼šè®­ç»ƒRQ-VAE tokenizer
â”œâ”€â”€ train_decoder.py        â­ ç¬¬äºŒé˜¶æ®µï¼šè®­ç»ƒæ¨èæ¨¡å‹
â”œâ”€â”€ modules/
â”‚   â”œâ”€â”€ rqvae.py           â­â­â­ RQ-VAEæ ¸å¿ƒå®ç°
â”‚   â”œâ”€â”€ quantize.py        â­â­â­ å‘é‡é‡åŒ–å±‚å®ç°
â”‚   â”œâ”€â”€ model.py           â­â­ Decoderæ¨èæ¨¡å‹
â”‚   â”œâ”€â”€ encoder.py         MLPç¼–ç å™¨/è§£ç å™¨
â”‚   â”œâ”€â”€ loss.py            æŸå¤±å‡½æ•°å®šä¹‰
â”‚   â””â”€â”€ tokenizer/
â”‚       â””â”€â”€ semids.py      â­â­ è¯­ä¹‰ID Tokenizer
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ processed.py       æ•°æ®é›†å¤„ç†
â”‚   â”œâ”€â”€ schemas.py         æ•°æ®ç»“æ„å®šä¹‰
â”‚   â””â”€â”€ amazon.py/ml1m.py  å…·ä½“æ•°æ®é›†å®ç°
â”œâ”€â”€ init/
â”‚   â””â”€â”€ kmeans.py          â­ KMeansåˆå§‹åŒ–ç æœ¬
â”œâ”€â”€ distributions/
â”‚   â””â”€â”€ gumbel.py          â­ Gumbel-Softmaxé‡‡æ ·
â””â”€â”€ configs/
    â”œâ”€â”€ rqvae_amazon.gin   RQ-VAEé…ç½®
    â””â”€â”€ decoder_amazon.gin Decoderé…ç½®
```

## ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

### é˜¶æ®µä¸€ï¼šRQ-VAEè®­ç»ƒ (train_rqvae.py)

```
ç‰©å“ç‰¹å¾(768ç»´embedding) 
    â†“ 
Encoder (MLP)
    â†“
latent embedding (32ç»´)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Residual Quantization (3å±‚)      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ Layer 1: embâ‚ = quantize(resâ‚€) â”‚ â”‚
â”‚  â”‚ resâ‚ = resâ‚€ - embâ‚             â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Layer 2: embâ‚‚ = quantize(resâ‚) â”‚ â”‚
â”‚  â”‚ resâ‚‚ = resâ‚ - embâ‚‚             â”‚ â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚
â”‚  â”‚ Layer 3: embâ‚ƒ = quantize(resâ‚‚) â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
è¯­ä¹‰ID: [idâ‚, idâ‚‚, idâ‚ƒ] (æ¯ä¸ªid âˆˆ [0, 255])
é‡åŒ–embedding: embâ‚ + embâ‚‚ + embâ‚ƒ
    â†“
Decoder (MLP)
    â†“
é‡å»ºç‰©å“ç‰¹å¾
```

**è®­ç»ƒç›®æ ‡**ï¼š
- **é‡å»ºæŸå¤±**ï¼šæœ€å°åŒ–è¾“å…¥ç‰¹å¾ä¸é‡å»ºç‰¹å¾çš„å·®å¼‚
- **é‡åŒ–æŸå¤±**ï¼šè®©encoderè¾“å‡ºæ¥è¿‘ç æœ¬å‘é‡ï¼ˆcommitment lossï¼‰

### é˜¶æ®µäºŒï¼šDecoderè®­ç»ƒ (train_decoder.py)

```
ç”¨æˆ·å†å²åºåˆ—: [itemâ‚, itemâ‚‚, ..., itemâ‚™]
    â†“
RQ-VAE Tokenizer (å†»ç»“)
    â†“
è¯­ä¹‰IDåºåˆ—: [[idâ‚Â¹, idâ‚‚Â¹, idâ‚ƒÂ¹], [idâ‚Â², idâ‚‚Â², idâ‚ƒÂ²], ...]
    â†“
Embedding + Position Encoding
    â†“
Transformer Encoder-Decoder
    â†“
è‡ªå›å½’ç”Ÿæˆä¸‹ä¸€ä¸ªç‰©å“çš„è¯­ä¹‰ID: [idâ‚â¿âºÂ¹, idâ‚‚â¿âºÂ¹, idâ‚ƒâ¿âºÂ¹]
```

## ğŸ”‘ æ ¸å¿ƒæ¨¡å—è¯¦è§£

### 1. RQ-VAE (modules/rqvae.py)

RQ-VAEæ˜¯**æ®‹å·®é‡åŒ–å˜åˆ†è‡ªç¼–ç å™¨**ï¼Œæ ¸å¿ƒæµç¨‹ï¼š
1. **ç¼–ç **ï¼šå°†é«˜ç»´ç‰©å“ç‰¹å¾å‹ç¼©åˆ°ä½ç»´ç©ºé—´
2. **æ®‹å·®é‡åŒ–**ï¼šå¤šå±‚é‡åŒ–ï¼Œæ¯å±‚å¤„ç†ä¸Šä¸€å±‚çš„æ®‹å·®
3. **è§£ç **ï¼šä»é‡åŒ–åçš„embeddingé‡å»ºåŸå§‹ç‰¹å¾

### 2. Quantizeå±‚ (modules/quantize.py)

å•å±‚é‡åŒ–çš„æ ¸å¿ƒæ­¥éª¤ï¼š
1. è®¡ç®—è¾“å…¥embeddingä¸ç æœ¬ä¸­æ‰€æœ‰å‘é‡çš„è·ç¦»
2. é€‰æ‹©æœ€è¿‘çš„ç æœ¬å‘é‡ä½œä¸ºé‡åŒ–ç»“æœ
3. ä½¿ç”¨Gumbel-Softmax / STE / Rotation Trickè¿›è¡Œå¯å¾®åˆ†è®­ç»ƒ

### 3. SemanticIdTokenizer (modules/tokenizer/semids.py)

å°†ç‰©å“è½¬æ¢ä¸ºè¯­ä¹‰IDï¼š
1. é¢„è®¡ç®—æ‰€æœ‰ç‰©å“çš„è¯­ä¹‰ID
2. å¤„ç†IDå†²çªï¼ˆå¤šä¸ªç‰©å“å¯èƒ½æ˜ å°„åˆ°ç›¸åŒIDï¼‰
3. æä¾›å‰ç¼€åŒ¹é…éªŒè¯ï¼ˆç”¨äºç”Ÿæˆæ—¶çš„çº¦æŸè§£ç ï¼‰

### 4. EncoderDecoderRetrievalModel (modules/model.py)

ç”Ÿæˆå¼æ¨èæ¨¡å‹ï¼š
1. ä½¿ç”¨Transformer Encoderå¤„ç†å†å²åºåˆ—ä¸Šä¸‹æ–‡
2. ä½¿ç”¨Transformer Decoderè‡ªå›å½’ç”Ÿæˆè¯­ä¹‰ID
3. æ”¯æŒçº¦æŸè§£ç ï¼ˆåªç”Ÿæˆå­˜åœ¨çš„è¯­ä¹‰IDå‰ç¼€ï¼‰

## ğŸ¯ å…³é”®æŠ€æœ¯ç‚¹

### Gumbel-Softmaxé‡å‚æ•°åŒ–

ä½¿ç¦»æ•£çš„é‡åŒ–æ“ä½œå˜å¾—å¯å¾®åˆ†ï¼Œå…è®¸æ¢¯åº¦åå‘ä¼ æ’­ï¼š
```python
# ä»Gumbel(0,1)é‡‡æ ·å¹¶æ·»åŠ åˆ°logits
y = logits + sample_gumbel(shape)
# æ¸©åº¦æ§åˆ¶çš„softmax
sample = softmax(y / temperature)
```

### KMeansåˆå§‹åŒ–ç æœ¬

ä½¿ç”¨KMeansèšç±»åˆå§‹åŒ–ç æœ¬å‘é‡ï¼ŒåŠ é€Ÿæ”¶æ•›ï¼š
```python
# å¯¹encoderè¾“å‡ºè¿›è¡ŒKMeansèšç±»
centroids = kmeans(encoder_outputs, k=codebook_size)
codebook.weight = centroids
```

### æ®‹å·®é‡åŒ– (Residual Quantization)

å¤šå±‚é‡åŒ–æ•è·ä¸åŒå±‚æ¬¡çš„ä¿¡æ¯ï¼š
```python
res = encoder(x)  # åˆå§‹æ®‹å·®
for layer in quantize_layers:
    emb = layer(res)      # é‡åŒ–å½“å‰æ®‹å·®
    res = res - emb       # è®¡ç®—æ–°æ®‹å·®
```

## ğŸ’» è¿è¡Œå‘½ä»¤

```bash
# 1. è®­ç»ƒRQ-VAE (ç”Ÿæˆç‰©å“çš„è¯­ä¹‰ID)
python train_rqvae.py configs/rqvae_amazon.gin

# 2. è®­ç»ƒDecoder (åºåˆ—æ¨èæ¨¡å‹)
python train_decoder.py configs/decoder_amazon.gin
```

## ğŸ“Š é…ç½®å‚æ•°è¯´æ˜

### RQ-VAEé…ç½® (configs/rqvae_amazon.gin)
```
vae_input_dim=768        # è¾“å…¥ç‰¹å¾ç»´åº¦
vae_embed_dim=32         # é‡åŒ–embeddingç»´åº¦  
vae_codebook_size=256    # ç æœ¬å¤§å°(æ¯å±‚IDçš„å–å€¼èŒƒå›´)
vae_n_layers=3           # é‡åŒ–å±‚æ•°(è¯­ä¹‰IDçš„é•¿åº¦)
```

### Decoderé…ç½®
```
decoder_embed_dim=64     # Decoder embeddingç»´åº¦
attn_heads=8             # æ³¨æ„åŠ›å¤´æ•°
attn_layers=4            # Transformerå±‚æ•°
```

## ğŸ”— å‚è€ƒè®ºæ–‡

1. [Recommender Systems with Generative Retrieval](https://arxiv.org/pdf/2305.05065) - TIGERåŸè®ºæ–‡
2. [Categorical Reparametrization with Gumbel-Softmax](https://openreview.net/pdf?id=rkE3y85ee) - Gumbel-SoftmaxæŠ€æœ¯
3. [Restructuring Vector Quantization with the Rotation Trick](https://arxiv.org/abs/2410.06424) - Rotation Trickä¼˜åŒ–
